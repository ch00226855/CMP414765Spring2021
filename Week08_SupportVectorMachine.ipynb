{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "Week08_SupportVectorMachine.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ch00226855/CMP414765Spring2021/blob/main/Week08_SupportVectorMachine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fv_i-KU5D8qD"
      },
      "source": [
        "# Week 8\n",
        "# Linear Support Vector Machines\n",
        "\n",
        "**Reading:** Textbook, Chapter 5.\n",
        "\n",
        "*Support Vector Machine* (SVM) is one of the most popular models in Machine Learning. It is capable of performing linear or nonlinear classification, regression, and even outlier detection. This lecture will explain the core concepts of SVMs, how to use them, and how they work.\n",
        "\n",
        "## Task 1: Linear SVM with Hard Margin\n",
        "- Each data example has two features: $x_1$ and $x_2$. Using them as coordinates, they can be visualized as a data point on the coordinate plane.\n",
        "- Binary classification: target value $y = 1$ means that the instance belongs to **Class 1 (the positive class)**; $y = -1$ means that the instance belongs to **Class -1 (the negative class)**.\n",
        "- Classes are **linearly separable**: The two classes can clearly be separrated with a straight line.\n",
        "- The **goal** is to find a straight line that best separates the two classes. No mis-classification is allowed.\n",
        "- The **best straight line** that separates the two classes is the one with maximized distance from it to the nearest data point on each class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa3BNuSsD8qN"
      },
      "source": [
        "<img src=\"https://github.com/ch00226855/CMP464-788-Spring2019/raw/master/Data/SVM1.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcIYsRMAD8qO"
      },
      "source": [
        "In the above graph, there are three lines (H1, H2, H3) that try to separate black dots from white dots.\n",
        "- H1 is clearly bad because it doesn't even separate the two classes properly.\n",
        "- H2 separates the two classes perfectly, but it is so close to the data points that it will probably not perform well on new instances.\n",
        "- H3 not only separates the two classes but also stays as far away from the closest training instances as possible. It is reasonable to believe that H3 will generalize well on new instances."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DS1rDYS7D8qO"
      },
      "source": [
        "<img src=\"https://github.com/ch00226855/CMP464-788-Spring2019/raw/master/Data/SVM2.jpeg\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKzp-dmaD8qP"
      },
      "source": [
        "Notice that adding more training examples \"off the street\" will not affect the decision boundary at all: it is fully determined (or \"supported\") by the points located on the edge of the street. These instances are called the **support vectors**.\n",
        "\n",
        "SVMs are sensitive to the feature scales, so proper feature scaling is necessary for obtaining a good decision boundary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67HyVn1dD8qP"
      },
      "source": [
        "## Express an SVM model with three parallel lines\n",
        "\n",
        "We can use three lines to expression the above binary classifier:\n",
        "\n",
        "- $w_1x_1 + w_2x_2 + b = 0$ represents the line in the middle of the gap.\n",
        "- $w_1x_1 + w_2x_2 + b = 1$ represents one boundary of the gap.\n",
        "- $w_1x_1 + w_2x_2 + b = -1$ represents the other boundary of the gap.\n",
        "\n",
        "The parameters of this model are $w_1(\\ge 0), w_2, b$. To avoid ambiguity, we choose $w_1$ to be non-negative. Here all three lines share the same slope $w$ because they are parallel. \n",
        "\n",
        "- For any point $(x_1, x_2)$ on the right half plane divided by the central line, the expression $w_1x_1 + w_2x_2 + b$ is positive.\n",
        "- For any point $(x_1, x_2)$ on the left half plane, $w_1x_1 + w_2x_2 + b$ is negative.\n",
        "\n",
        "**Question**: What is the slope of these lines?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sktgvgvAD8qR"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5dwYHjAD8qS"
      },
      "source": [
        "# Suppose w1 = 1, w2 = 2, and b = -3, draw the above\n",
        "# three lines on a graph.\n",
        "w1, w2, b = 1, 2, -3\n",
        "x1_coordinates = np.array([-5, 5])\n",
        "\n",
        "# plot w1 * x1 + w2 * x2 + b = 0\n",
        "\n",
        "\n",
        "# plot w1 * x1 + w2 *x2 + b = 1\n",
        "\n",
        "\n",
        "# plot w1 * x1 + w2 *x2 + b = -1\n",
        "\n",
        "\n",
        "\n",
        "plt.xlim([-3, 3])\n",
        "plt.ylim([0, 5])\n",
        "plt.title(\"Original Curve\")\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HZXhj8_D8qW"
      },
      "source": [
        "## Linear SVM: Decision function and predictions\n",
        "\n",
        "The linear SVM classifier model predicts the class of a new instance $(x_1, x_2)$ by simply computing the decision function $w_1x_1 + w_2x_2 + b$: if the result is positive, the predicted class is the positive class, otherwise it is predicted to belong to the negative class.\n",
        "\n",
        "**Decision rule**\n",
        "\\begin{equation}\n",
        "\\hat{y}(prediction)  = \n",
        "\\left\\{\n",
        "\\begin{array}{cc}\n",
        " 1 & \\textit{if } w_1x_1 + w_2x_2 + b \\ge 0,\\\\\n",
        " -1 & \\textit{if } w_1x_1 + w_2x_2 + b < 0.\n",
        "\\end{array}\n",
        "\\right.\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIIhZWtqD8qW"
      },
      "source": [
        "# Suppose w1 = 1, w2 = 2, and b = -3, decide the \n",
        "# class of: \n",
        "# 1) x_1 = 0, x_2 = 1; \n",
        "# 2) x_1 = 2, x_2 = 3;\n",
        "# 3) x_1 = 1, x_2 = 1.1.\n",
        "\n",
        "w1, w2, b = 1, 2, -3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i--WrPKvFsFE"
      },
      "source": [
        "# Write a function predict_SVM(w1, w2, b, x1, x2) that\n",
        "# returns the class prediction.\n",
        "\n",
        "def predict_SVM(w1, w2, b, x1, x2):\n",
        "    \n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZmfGItDF4nP"
      },
      "source": [
        "# Apply predict_SVM() to these cases:\n",
        "# 2)\n",
        "x1, x2 = 2, 3\n",
        "\n",
        "\n",
        "# 3)\n",
        "x1, x2 = 1, 1.1\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y05m5qMBD8qY"
      },
      "source": [
        "# Plot the points together with three decision lines.\n",
        "\n",
        "# Suppose w1 = 1, w2 = 2, and b = -3, draw the above\n",
        "# three lines on a graph.\n",
        "w1, w2, b = 1, 2, -3\n",
        "x1_coordinates = np.array([-5, 5])\n",
        "\n",
        "# w1 * x1 + w2 * x2 + b = 0\n",
        "x2_coordinates = (-w1 * x1_coordinates - b) / w2\n",
        "plt.plot(x1_coordinates, x2_coordinates, 'b-', label=\"x1 + 2x2 - 3 = 0\")\n",
        "\n",
        "# w1 * x1 + w2 *x2 + b = 1\n",
        "x2_coordinates_1 = (-w1 * x1_coordinates - b + 1) / w2\n",
        "plt.plot(x1_coordinates, x2_coordinates_1, 'g--', label=\"x1 + 2x2 - 3 = 1\")\n",
        "\n",
        "# w1 * x1 + w2 *x2 + b = -1\n",
        "x2_coordinates_1 = (-w1 * x1_coordinates - b - 1) / w2\n",
        "plt.plot(x1_coordinates, x2_coordinates_1, 'm--', label=\"x1 + 2x2 - 3 = -1\")\n",
        "\n",
        "# Plot the above three points\n",
        "x1 = [0, 2, 1]\n",
        "x2 = [1, 3, 1.1]\n",
        "plt.plot(x1, x2, 'm+')\n",
        "\n",
        "plt.xlim([-3, 3])\n",
        "plt.ylim([0, 5])\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wgvBKVSD8qZ"
      },
      "source": [
        "## Linear SVM: Cost Function\n",
        "In order to understand how to measure the performance of an SVM model, Let's first explore how the magnitude of parameters affects the margin."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osD2xn1RD8qa"
      },
      "source": [
        "# In the previous model, increase w1 from 1 to 2,\n",
        "# and plot the new decision margin.\n",
        "# lines are: 2 * x1 + 2 * x2 - 3 = (0, 1, -1)\n",
        "\n",
        "w1, w2, b = 2, 2, -3\n",
        "x1_coordinates = np.array([-5, 5])\n",
        "# plot w1 * x1 + w2 * x2 + b = 0\n",
        "\n",
        "\n",
        "# plot w1 * x1 + w2 *x2 + b = 1\n",
        "\n",
        "\n",
        "# plot w1 * x1 + w2 *x2 + b = -1\n",
        "\n",
        "plt.title(r\"Increase $w_1$\")\n",
        "plt.xlim([-3, 3])\n",
        "plt.ylim([0, 5])\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "feiYK-LSD8qb"
      },
      "source": [
        "# Decrease w2 from 2 to 1 from the original model,\n",
        "# and plot the new decision margin.\n",
        "\n",
        "w1, w2, b = 1, 1, -3\n",
        "x1_coordinates = np.array([-5, 5])\n",
        "\n",
        "# Plot the lines\n",
        "\n",
        "\n",
        "plt.title(r\"Decrease w2\")\n",
        "plt.xlim([-3, 3])\n",
        "plt.ylim([0, 5])\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "MXdR0shhD8qc"
      },
      "source": [
        "# Change b from -3 to -2 from the original model,\n",
        "# and plot the new decision margin\n",
        "\n",
        "w1, w2, b = 1, 2, -2\n",
        "x1_coordinates = np.array([-5, 5])\n",
        "\n",
        "# Plot the lines\n",
        "\n",
        "\n",
        "plt.title(r\"Increase b\")\n",
        "plt.xlim([-3, 3])\n",
        "plt.ylim([0, 5])\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkF9UA5vD8qd"
      },
      "source": [
        "**How does each parameter affect the width of decision margin?**\n",
        "- smaller w_1 -> wider gap; larger w_1 -> narrower gap.\n",
        "- smaller w_2 -> wider gap; larger w_2 -> narrower gap.\n",
        "- b does not affect the width of the gap.\n",
        "\n",
        "### Cost Function\n",
        "- Cost function should give a large value if the gap is narrow, and gives a small value if the gap is wide.\n",
        "- Cost function $= w_1^2 + w_2^2$, subject to no point is placed inside the gap.\n",
        "- If a point is inside the gap, then $w_1  x_1 + w_2  x_2 + b$ will be less than 1 and greater than -1.\n",
        "- The above property can be tranlated into $y(w_1x_1 + w_2x_2 + b) < 1$.\n",
        "- The choice of cost function: $J(w_1, w_2, b) = w_1^2 + w_2^2$, subject to $y(w_1x_1 + w_2x_2 + b) < 1$ for all training instances."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnqNAfskD8qe"
      },
      "source": [
        "## Drawbacks of Hard Margin SVM\n",
        "If we strictly impose that all instances be off the street and on the correct side, the model can only be applied to models that are linearly separable. Moreover, it will be very sensitive to outliers. The above figure illustrates how badly one outlier may affect the model. \n",
        "\n",
        "<img src=\"https://github.com/ch00226855/CMP464-788-Spring2019/raw/master/Data/SVM3.png\">\n",
        "\n",
        "To avoid these issues, it is preferable to use a more flexible model. The objective is to find a good balance between keeping the street as wide as possible and limiting the margin violations. This is called *soft margin classification*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HI1UH9dbD8qe"
      },
      "source": [
        "## Task 2: Linear SVM with Soft Margin\n",
        "- Each data example has two features: $x_1$ and $x_2$.\n",
        "- Binary classification: target value $y = 1$ means that the instance belongs to \"class 1\", $y = -1$ means that the instance belongs to \"class -1\".\n",
        "- Classes are **mostly** linearly separable: **with a few exceptions**, the two classes can be separrated with a straight line.\n",
        "- The goal is to find a straight line that best separates the two classes. **Mis-classifications are allowed, but each mis-classification will add a cost to the model.**\n",
        "- The objective function takes into account both **the magnitude of w's (how wide the gap is) and the degree of margin violations**.\n",
        "\n",
        "<img src=\"https://github.com/ch00226855/CMP464-788-Spring2019/raw/master/Data/SVM4.jpg\" width=\"600\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVnsbG6WJ3OC"
      },
      "source": [
        "### Cost Function\n",
        "\n",
        "$J(w_1, w_2, b)$ \n",
        "$= \\frac{1}{2}(w_1^2+w_2^2) + C\n",
        "\\sum_{i=1}^m\\max(0, 1-y^{(i)}(w_1x_1^{(i)}+w_2x_2^{(i)}+b))$\n",
        "\n",
        "- The first sum in the cost function will push the model to have small weights, leading to a larger margin.\n",
        "- The second sum computes the total of all margin violations. An instance's margin violation is equal to 0 if it is located off the street and on the correct side, or else it is proportional to the distance to the correct side of the street.\n",
        "- Minimizing this term ensures that the model makes the margin violations as small and as few as possible.\n",
        "\n",
        "### Hinge loss function\n",
        "The function $\\max(0, 1-t)$ used in the above cost function is called the *hinge loss* function. \n",
        "- It is equal to 0 when $t\\ge 1$. \n",
        "- Its derivative (slope) is equal to 0 if $t>1$.\n",
        "- Its derivative (slope) is equal to -1 if $t<1$.\n",
        "- It is a convex function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frjxiks3J_7U"
      },
      "source": [
        "# Exercise: Plot the graph of the hinge function\n",
        "# h(t) = max(0, 1 - t) \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcuGN_jJD8qf"
      },
      "source": [
        "### Advantages of Soft Margin SVM\n",
        "- The dataset does not have to be linearly separable.\n",
        "- Outliers does not affect the model too much.\n",
        "- The cost function is convex and it has no constraints, thus gradient descent can be applied to its minimization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FD_b4z3D8qf"
      },
      "source": [
        "## Improving Linear SVM\n",
        "- Feature scaling: are all features distributed similarly?\n",
        "- $C$: how much does penalty matter?\n",
        "- Class weights: are all classes equally important?\n",
        "- Multiple classes: One vs. One, or One vs. Rest?\n",
        "- **Kernel SVM**: Allow non-linear decision boundaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRCCpCqcD8qj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxPqQU7KD8qk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}